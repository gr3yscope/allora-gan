import os
import pandas as pd
from datetime import datetime, timedelta
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from config import data_base_path
import random
import requests
import retrying
import joblib

forecast_price = {}

binance_data_path = os.path.join(data_base_path, "binance/futures-klines")
MAX_DATA_SIZE = 1000
INITIAL_FETCH_SIZE = 1000

@retrying.retry(wait_exponential_multiplier=1000, wait_exponential_max=10000, stop_max_attempt_number=5)
def fetch_prices(symbol, interval="1m", limit=1000, start_time=None, end_time=None):
    try:
        base_url = "https://fapi.binance.com"
        endpoint = f"/fapi/v1/klines"
        params = {
            "symbol": symbol,
            "interval": interval,
            "limit": limit
        }
        if start_time:
            params['startTime'] = start_time
        if end_time:
            params['endTime'] = end_time

        url = base_url + endpoint
        response = requests.get(url, params=params)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f'Failed to fetch prices for {symbol} from Binance API: {str(e)}')
        raise e

def download_data(token):
    symbols = f"{token.upper()}USDT"
    interval = "5m"
    current_datetime = datetime.now()
    download_path = os.path.join(binance_data_path, token.lower())
    
    file_path = os.path.join(download_path, f"{token.lower()}_5m_data.csv")

    if os.path.exists(file_path):
        start_time = int((current_datetime - timedelta(minutes=500)).timestamp() * 1000)
        end_time = int(current_datetime.timestamp() * 1000)
        new_data = fetch_prices(symbols, interval, 100, start_time, end_time)
    else:
        start_time = int((current_datetime - timedelta(minutes=INITIAL_FETCH_SIZE*5)).timestamp() * 1000)
        end_time = int(current_datetime.timestamp() * 1000)
        new_data = fetch_prices(symbols, interval, INITIAL_FETCH_SIZE, start_time, end_time)

    new_df = pd.DataFrame(new_data, columns=[
        "start_time", "open", "high", "low", "close", "volume", "close_time",
        "quote_asset_volume", "number_of_trades", "taker_buy_base_asset_volume", 
        "taker_buy_quote_asset_volume", "ignore"
    ])

    if os.path.exists(file_path):
        old_df = pd.read_csv(file_path)
        combined_df = pd.concat([old_df, new_df])
        combined_df = combined_df.drop_duplicates(subset=['start_time'], keep='last')
    else:
        combined_df = new_df

    if len(combined_df) > MAX_DATA_SIZE:
        combined_df = combined_df.iloc[-MAX_DATA_SIZE:]

    if not os.path.exists(download_path):
        os.makedirs(download_path)
    combined_df.to_csv(file_path, index=False)
    print(f"Updated data for {token} saved to {file_path}. Total rows: {len(combined_df)}")

def format_data(token):
    path = os.path.join(binance_data_path, token.lower())
    file_path = os.path.join(path, f"{token.lower()}_5m_data.csv")

    if not os.path.exists(file_path):
        print(f"No data file found for {token}")
        return

    df = pd.read_csv(file_path)

    columns_to_use = [
        "start_time", "open", "high", "low", "close", "volume",
        "close_time", "quote_asset_volume", "number_of_trades",
        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume"
    ]

    if set(columns_to_use).issubset(df.columns):
        df = df[columns_to_use]
        df.columns = [
            "start_time", "open", "high", "low", "close", "volume",
            "end_time", "quote_asset_volume", "n_trades", 
            "taker_volume", "taker_volume_usd"
        ]
        df.index = pd.to_datetime(df["start_time"], unit='ms')
        df.index.name = "date"

        output_path = os.path.join(data_base_path, f"{token.lower()}_price_data.csv")
        df.sort_index().to_csv(output_path)
        print(f"Formatted data saved to {output_path}")
    else:
        print(f"Required columns are missing in {file_path}. Skipping this file.")

# Train model with RandomizedSearchCV for hyperparameter tuning
def train_model(token):
    time_start = datetime.now()

    price_data = pd.read_csv(os.path.join(data_base_path, f"{token.lower()}_price_data.csv"))
    df = pd.DataFrame()

    price_data["date"] = pd.to_datetime(price_data["date"])
    price_data.set_index("date", inplace=True)
    df = price_data.resample('10T').mean()

    df = df.dropna()
    X = np.arange(len(df)).reshape(-1, 1)
    y = df['close'].values

    # Pipeline for Random Forest with StandardScaler
    rf_pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Standardize features
        ('rf', RandomForestRegressor(random_state=0))  # RandomForest
    ])

    # Hyperparameter tuning using RandomizedSearchCV for Random Forest
    param_dist_rf = {
        'rf__n_estimators': [100, 150],
        'rf__max_depth': [None, 10, 20],
        'rf__min_samples_split': [2, 4],
        'rf__min_samples_leaf': [1, 2],
        'rf__bootstrap': [True, False]
    }

    random_search_rf = RandomizedSearchCV(rf_pipeline, param_distributions=param_dist_rf, 
                                          n_iter=20, cv=5, scoring='neg_mean_squared_error', n_jobs=2, random_state=0)
    random_search_rf.fit(X, y)
    
    # Get the best model from RandomizedSearchCV
    best_rf_model = random_search_rf.best_estimator_
    print(f"Best parameters for Random Forest: {random_search_rf.best_params_}")

    # Save the trained Random Forest model
    joblib.dump(best_rf_model, f"models/{token}_random_forest_model.pkl")
    print(f"Random Forest model saved for {token}")

    # Evaluate Random Forest
    rf_predicted_price = best_rf_model.predict(X)
    mse_rf = mean_squared_error(y, rf_predicted_price)
    r2_rf = r2_score(y, rf_predicted_price)
    print(f"Random Forest MSE: {mse_rf}, R²: {r2_rf}")

    # Pipeline for Gradient Boosting with StandardScaler
    gb_pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Standardize features
        ('gb', GradientBoostingRegressor(random_state=0))  # GradientBoosting
    ])

    # Hyperparameter tuning using RandomizedSearchCV for Gradient Boosting
    param_dist_gb = {
        'gb__n_estimators': [100, 150],
        'gb__learning_rate': [0.01, 0.1],
        'gb__max_depth': [3, 5],
    }

    random_search_gb = RandomizedSearchCV(gb_pipeline, param_distributions=param_dist_gb, 
                                          n_iter=20, cv=5, scoring='neg_mean_squared_error', n_jobs=2, random_state=0)
    random_search_gb.fit(X, y)

    # Get the best model from RandomizedSearchCV
    best_gb_model = random_search_gb.best_estimator_
    print(f"Best parameters for Gradient Boosting: {random_search_gb.best_params_}")

    # Save the trained Gradient Boosting model
    joblib.dump(best_gb_model, f"models/{token}_gradient_boosting_model.pkl")
    print(f"Gradient Boosting model saved for {token}")

    # Evaluate Gradient Boosting
    gb_predicted_price = best_gb_model.predict(X)
    mse_gb = mean_squared_error(y, gb_predicted_price)
    r2_gb = r2_score(y, gb_predicted_price)
    print(f"Gradient Boosting MSE: {mse_gb}, R²: {r2_gb}")

    # Combine predictions from both models
    final_predicted_price = (rf_predicted_price[-1] + gb_predicted_price[-1]) / 2

    fluctuation_range = 0.01 * final_predicted_price
    min_price = final_predicted_price - fluctuation_range
    max_price = final_predicted_price + fluctuation_range

    price_predict = random.uniform(min_price, max_price)
    forecast_price[token] = price_predict

    print(f"Forecasted price for {token}: {forecast_price[token]}")

    time_end = datetime.now()
    print(f"Time elapsed forecast: {time_end - time_start}")

def update_data():
    tokens = ["ETH", "BTC", "BNB", "SOL", "ARB"]
    for token in tokens:
        download_data(token)
        format_data(token)
        train_model(token)

if __name__ == "__main__":
    update_data()



-----------------------------------------------------------------------


import json
import os
import joblib
from flask import Flask, Response
import numpy as np

app = Flask(__name__)

# Path ke direktori tempat menyimpan model yang telah dilatih
MODEL_DIR = "models"

# Fungsi untuk memuat model dan scaler
def load_model_and_scaler(token, model_type):
    """Memuat model dan scaler dari file .pkl"""
    model_path = os.path.join(MODEL_DIR, f"{token}_{model_type}_model.pkl")
    scaler_path = os.path.join(MODEL_DIR, f"{token}_scaler.pkl")
    
    if os.path.exists(model_path) and os.path.exists(scaler_path):
        model = joblib.load(model_path)
        scaler = joblib.load(scaler_path)
        return model, scaler
    else:
        print(f"Model atau scaler untuk token {token} tidak ditemukan.")
        return None, None

# Fungsi untuk melakukan inferensi (prediksi)
def get_token_inference(token):
    """Melakukan inferensi menggunakan model yang sudah dilatih dan disimpan."""
    try:
        # Memuat model Random Forest dan Gradient Boosting
        rf_model, scaler = load_model_and_scaler(token, "random_forest")
        gb_model, _ = load_model_and_scaler(token, "gradient_boosting")
        
        if rf_model is None or gb_model is None or scaler is None:
            return {"error": f"Model untuk token {token} tidak tersedia"}
        
        # Misalkan kita gunakan data terbaru (prediksi terakhir) untuk inferensi
        last_time_index = np.array([[100]])  # Contoh input sederhana
        last_time_index_scaled = scaler.transform(last_time_index)

        # Prediksi menggunakan kedua model dan gabungkan hasilnya
        rf_pred = rf_model.predict(last_time_index_scaled)[0]
        gb_pred = gb_model.predict(last_time_index_scaled)[0]
        combined_pred = (rf_pred + gb_pred) / 2

        return combined_pred
    except Exception as e:
        print(f"Error dalam prediksi: {e}")
        return {"error": str(e)}

@app.route("/inference/<string:token>")
def generate_inference(token):
    """Generate inference for given token."""
    if not token or token not in ["ETH", "BTC", "SOL"]:
        error_msg = "Token is required" if not token else "Token not supported"
        return Response(json.dumps({"error": error_msg}), status=400, mimetype='application/json')

    try:
        inference = get_token_inference(token)
        return Response(json.dumps({"prediction": inference}), status=200, mimetype='application/json')
    except Exception as e:
        return Response(json.dumps({"error": str(e)}), status=500, mimetype='application/json')

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8011)
